{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591a0ee7-e22f-4fb2-9778-4c8e296eb246",
   "metadata": {},
   "source": [
    "# Stellar Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f82db-e0b5-40e8-b8b2-c67efbd963fd",
   "metadata": {},
   "source": [
    "Our project is based on the benefit from machine augmentation to discern differences among stars, providing insights into the universe's composition beyond the amount of data that most individual astronomers would not be capable of classifying themselves. \n",
    "Stellar classification is a revised system on distinguishing stars through what before was simply a measure of brightness, magnitude or gravity. This enhanced method splits a star into a prism (diffracting light into several beams) and uses atomic energy-level based lines representing different element strengths to separate it. \n",
    "Using spectral features for stellar categorisation is now a normality, with the two most current schemes being a combination of the Harvard System (temperature) ‘developed at Harvard Observatory in the early 20th century’   and the MK system (luminosity).   \n",
    "In this instance we are using a dataset that relies on a similar SDSS photometric measuring system to capture the spectral energy of stellar objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff3f8a-0f9a-48e9-9ef4-4c1e5a09b79c",
   "metadata": {},
   "source": [
    "## 1.1.1 Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4638dd-0ca3-4bae-a924-d4650a855657",
   "metadata": {},
   "source": [
    "Astronomers find the analysis of stellar spectra to be fundamental in understanding the composition and assets of stars. Our primary goal is to make a machine that can automate these repetitive tasks through extensive amounts of data, advancing the human dimension of space exploration through artificial intelligence. To reach these goals we will need to answer:\n",
    "\n",
    "•\tWhat models are suitable for this goal?\n",
    "\n",
    "•\tHow should we build these models?\n",
    "\n",
    "•\tWhich methods are fitting for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2119271-b6cf-4ea9-82e9-8ddefd567f55",
   "metadata": {},
   "source": [
    "## 1.1.2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd57f4-867a-4f79-a6f5-4676071fb4a7",
   "metadata": {},
   "source": [
    "This Stellar Classification dataset is provided by Fedesoriano, a data scientist at Kaggle. Kaggle hosts an extensive collection of datasets for use by their online communities. The raw dataset consists of 100,00 samples detailed by 17 features, making it quite large.\n",
    "For our project we will be using Spyder, as it is a useful representation of graphics and diagrams as well as basic console outputs, the dataset is also very large and therefore benefits from being processed in Python 3 using the PANDAS package.\n",
    "Additionally, frameworks such as Sckit-Learn (Machine learning framework for Python), TensorFlow (Google’s deep learning framework) and Keras (high level neural networks API) will be used.\n",
    "\n",
    "In terms of evaluation metrics, we will primarily be looking at the accuracy, which is the ratio of correct predictions to the entire number of predictions made.\n",
    "\n",
    "Accuracy = Number of correct predictions/ Total number of predications made\n",
    "\n",
    "This metric works best when there is an equal number of samples per classification. If this is not the case then we will need to consider fine tuning through resampling techniques (oversampling or under sampling) and as much anomaly detection as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083f65-f396-450d-9746-2ed0f726072d",
   "metadata": {},
   "source": [
    "## 1.2.1 Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de550fb-97f9-43d4-9962-7d9f7da0c8f7",
   "metadata": {},
   "source": [
    "This dataset is mainly carved from five specific filters that measure the flux (brightness) of objects in different categories:\n",
    "\n",
    "–\tu - ultraviolet light, shorter wavelengths. \n",
    "\n",
    "–\tg - green light, also sensitive to red and blue portions of light.\n",
    "\n",
    "–\tr - red light, solely capturing red.\n",
    "\n",
    "–\ti - infrared light.\n",
    "\n",
    "–\tz - captures near infrared light, in the infrared portion of the photometric system.\n",
    "\n",
    "•\tAdd. numeric features\n",
    "\n",
    "–\tobj_ID - This is a unique number to identify every individual image.\n",
    "\n",
    "–\talpha - The right ascension angle (akin to longitude)\n",
    "\n",
    "–\tdelta - The declination angle (akin to latitude)\n",
    "\n",
    "–\trun_ID - The run number used to identify different scans conducted by a specific survey.\n",
    "\n",
    "–\trerun_ID - A number attached to the specification of how the image was processed.\n",
    "\n",
    "–\tcam_col - Stands for ‘camera column’ and is a number to identify the scanline within the run.\n",
    "\n",
    "–\tfield_ID - A number to identify each field.\n",
    "\n",
    "–\tspec_obj_ID - A spectroscopic object’s unique ID.\n",
    "\n",
    "–\tredshift - A number associated to the redshift, this is based on an increase in wavelength.\n",
    "\n",
    "–\tplate - An identifier for each plate in SDSS.\n",
    "\n",
    "–\tMJD - Stands for ‘Modified Julian Date’, which is used to describe when a specific part of the SDSS data was taken.\n",
    "\n",
    "–\tfiber_ID - refers to the identifier of the fiber directing light to the focal plane in each observation.\n",
    "\n",
    "The only descriptive feature in the dataset is the resulting class of either Galaxy, QSO (Quasar) and Star.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df210b2c-590d-4802-bc5f-e8e3eb550c06",
   "metadata": {},
   "source": [
    "## 1.2.2 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8554e18-38a6-458d-93a6-46f27b2c8388",
   "metadata": {},
   "source": [
    "Through exploratory data analysis through .head() and .tail(), the original data contained surprisingly few missing samples, and therefore no field omitting was needed. \n",
    "As a preprocessing step, the distribution percentage of each class was displayed and immediately showed quite an imbalance of class samples (mainly Galaxy with a more minor imbalance between QSO and Stars), which would work against the accuracy metric. As a result the SMOTE (Synthetic Minority Over-sampling Technique) operation  was used for balancing as shown in the figure below.\n",
    "\n",
    "omitted Alysha's figure as we can put the code here to display\n",
    "\n",
    "Alongside this a statistical summary was visualised to spot any anomalies and outliers. Initially boxplots were used as a discovery tool,  but some data did not display well due to the extreme outliers. \n",
    "A better representation of the data before addressing the outliers came in the format of histograms, which were used to show the plots before and after they were standardised using the Z – Score measurement. The equation for this is as follows: \n",
    "\n",
    "Z standardisation=(x-μ)/σ\n",
    "\n",
    "With μ being equal to the mean and σ being equal to the standard deviation.\n",
    "\n",
    "The figure below shows an example of what happened to most feature variables before and after Z-score normalisation.\n",
    "\n",
    "omitted another figure\n",
    "\n",
    "It had more of a detrimental effect due to the extreme outliers, and so another method for tidying up the features was used.\n",
    "Robust scaling  was more efficient as it uses median and interquartile range to normalise data rather than the mean, meaning it is less sensitive to outliers. This had a much more positive impact as shown in the figure below.\n",
    "\n",
    "Robust scale (x)=  (x-median(x))/(Q3(x)-Q1(x))\n",
    "\n",
    "omitted another figure\n",
    "\n",
    "In order to more swiftly train the model, variables that had a lesser outcome on the class would be considered as secondary features. There seemed to be much more emphasis on more primary ones such as ‘u’, ‘g’, ‘r’ , ‘i’, ‘z’, ‘alpha’, ‘delta’, ‘plate’ and especially ‘redshift’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a38f-d8c2-4960-876d-e8881e04c218",
   "metadata": {},
   "source": [
    "## 1.2.3 Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c22cce-47b0-4a70-87ab-e96e56b3eeb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Coursework/star_classification.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoursework/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m filename_read \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstar_classification.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_read\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#renaming the columns to more understandable heading\u001b[39;00m\n\u001b[1;32m     24\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreen Light\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Coursework/star_classification.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "# Reads dataset and puts it in a dataframe\n",
    "path = \"Coursework/\"\n",
    "filename_read = os.path.join(path, \"star_classification.csv\")\n",
    "df = pd.read_csv(filename_read)\n",
    "\n",
    "#renaming the columns to more understandable heading\n",
    "df = df.rename(columns = {'g': 'Green Light'})\n",
    "df = df.rename(columns = {'u': 'Ultraviolet Light'})\n",
    "df = df.rename(columns = {'r': 'Red Light'})\n",
    "df = df.rename(columns = {'i': 'Infrared Light'})\n",
    "df = df.rename(columns = {'delta': 'Declination Angle'})\n",
    "df = df.rename(columns = {'alpha': 'Ascension Angle'})\n",
    "\n",
    "# Selecting the 'redshift' column for scaling\n",
    "redshift_data = data[['redshift']]\n",
    "\n",
    "# Creating a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting and transforming the 'redshift' data\n",
    "scaled_redshift = scaler.fit_transform(redshift_data)\n",
    "\n",
    "# Adding the scaled redshift back to the dataframe\n",
    "data['scaled_redshift'] = scaled_redshift\n",
    "\n",
    "# Displaying the first few rows of the updated dataframe\n",
    "data.head()\n",
    "\n",
    "\n",
    "#displaying the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4116d2c-cd04-46cd-86e8-fd68f6631707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b764cc-5afe-46ee-b5bc-da9e4d330c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
