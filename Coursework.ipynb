{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591a0ee7-e22f-4fb2-9778-4c8e296eb246",
   "metadata": {},
   "source": [
    "# Stellar Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f82db-e0b5-40e8-b8b2-c67efbd963fd",
   "metadata": {},
   "source": [
    "Our project is based on the benefit from machine augmentation to discern differences among stars, providing insights into the universe's composition beyond the amount of data that most individual astronomers would not be capable of classifying themselves. \n",
    "Stellar classification is a revised system on distinguishing stars through what before was simply a measure of brightness, magnitude or gravity. This enhanced method splits a star into a prism (diffracting light into several beams) and uses atomic energy-level based lines representing different element strengths to separate it. \n",
    "Using spectral features for stellar categorisation is now a normality, with the two most current schemes being a combination of the Harvard System (temperature) ‘developed at Harvard Observatory in the early 20th century’   and the MK system (luminosity).   \n",
    "In this instance we are using a dataset that relies on a similar SDSS photometric measuring system to capture the spectral energy of stellar objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff3f8a-0f9a-48e9-9ef4-4c1e5a09b79c",
   "metadata": {},
   "source": [
    "## 1.1.1 Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4638dd-0ca3-4bae-a924-d4650a855657",
   "metadata": {},
   "source": [
    "Astronomers find the analysis of stellar spectra to be fundamental in understanding the composition and assets of stars. Our primary goal is to make a machine that can automate these repetitive tasks through extensive amounts of data, advancing the human dimension of space exploration through artificial intelligence. To reach these goals we will need to answer:\n",
    "\n",
    "•\tWhat models are suitable for this goal?\n",
    "\n",
    "•\tHow should we build these models?\n",
    "\n",
    "•\tWhich methods are fitting for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2119271-b6cf-4ea9-82e9-8ddefd567f55",
   "metadata": {},
   "source": [
    "## 1.1.2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd57f4-867a-4f79-a6f5-4676071fb4a7",
   "metadata": {},
   "source": [
    "This Stellar Classification dataset is provided by Fedesoriano, a data scientist at Kaggle. Kaggle hosts an extensive collection of datasets for use by their online communities. The raw dataset consists of 100,00 samples detailed by 17 features, making it quite large.\n",
    "For our project we will be using Spyder, as it is a useful representation of graphics and diagrams as well as basic console outputs, the dataset is also very large and therefore benefits from being processed in Python 3 using the PANDAS package.\n",
    "Additionally, frameworks such as Sckit-Learn (Machine learning framework for Python), TensorFlow (Google’s deep learning framework) and Keras (high level neural networks API) will be used.\n",
    "\n",
    "In terms of evaluation metrics, we will primarily be looking at the accuracy, which is the ratio of correct predictions to the entire number of predictions made.\n",
    "\n",
    "Accuracy = Number of correct predictions/ Total number of predications made\n",
    "\n",
    "This metric works best when there is an equal number of samples per classification. If this is not the case then we will need to consider fine tuning through resampling techniques (oversampling or under sampling) and as much anomaly detection as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083f65-f396-450d-9746-2ed0f726072d",
   "metadata": {},
   "source": [
    "## 1.2.1 Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de550fb-97f9-43d4-9962-7d9f7da0c8f7",
   "metadata": {},
   "source": [
    "This dataset is mainly carved from five specific filters that measure the flux (brightness) of objects in different categories:\n",
    "\n",
    "–\tu - ultraviolet light, shorter wavelengths. \n",
    "\n",
    "–\tg - green light, also sensitive to red and blue portions of light.\n",
    "\n",
    "–\tr - red light, solely capturing red.\n",
    "\n",
    "–\ti - infrared light.\n",
    "\n",
    "–\tz - captures near infrared light, in the infrared portion of the photometric system.\n",
    "\n",
    "•\tAdd. numeric features\n",
    "\n",
    "–\tobj_ID - This is a unique number to identify every individual image.\n",
    "\n",
    "–\talpha - The right ascension angle (akin to longitude)\n",
    "\n",
    "–\tdelta - The declination angle (akin to latitude)\n",
    "\n",
    "–\trun_ID - The run number used to identify different scans conducted by a specific survey.\n",
    "\n",
    "–\trerun_ID - A number attached to the specification of how the image was processed.\n",
    "\n",
    "–\tcam_col - Stands for ‘camera column’ and is a number to identify the scanline within the run.\n",
    "\n",
    "–\tfield_ID - A number to identify each field.\n",
    "\n",
    "–\tspec_obj_ID - A spectroscopic object’s unique ID.\n",
    "\n",
    "–\tredshift - A number associated to the redshift, this is based on an increase in wavelength.\n",
    "\n",
    "–\tplate - An identifier for each plate in SDSS.\n",
    "\n",
    "–\tMJD - Stands for ‘Modified Julian Date’, which is used to describe when a specific part of the SDSS data was taken.\n",
    "\n",
    "–\tfiber_ID - refers to the identifier of the fiber directing light to the focal plane in each observation.\n",
    "\n",
    "The only descriptive feature in the dataset is the resulting class of either Galaxy, QSO (Quasar) and Star.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df210b2c-590d-4802-bc5f-e8e3eb550c06",
   "metadata": {},
   "source": [
    "## 1.2.2 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8554e18-38a6-458d-93a6-46f27b2c8388",
   "metadata": {},
   "source": [
    "Through exploratory data analysis through .head() and .tail(), the original data contained surprisingly few missing samples, and therefore no field omitting was needed. \n",
    "As a preprocessing step, the distribution percentage of each class was displayed and immediately showed quite an imbalance of class samples (mainly Galaxy with a more minor imbalance between QSO and Stars), which would work against the accuracy metric. As a result the SMOTE (Synthetic Minority Over-sampling Technique) operation  was used for balancing as shown in the figure below.\n",
    "\n",
    "omitted Alysha's figure as we can put the code here to display\n",
    "\n",
    "Alongside this a statistical summary was visualised to spot any anomalies and outliers. Initially boxplots were used as a discovery tool,  but some data did not display well due to the extreme outliers. \n",
    "A better representation of the data before addressing the outliers came in the format of histograms, which were used to show the plots before and after they were standardised using the Z – Score measurement. The equation for this is as follows: \n",
    "\n",
    "Z standardisation=(x-μ)/σ\n",
    "\n",
    "With μ being equal to the mean and σ being equal to the standard deviation.\n",
    "\n",
    "The figure below shows an example of what happened to most feature variables before and after Z-score normalisation.\n",
    "\n",
    "omitted another figure\n",
    "\n",
    "It had more of a detrimental effect due to the extreme outliers, and so another method for tidying up the features was used.\n",
    "Robust scaling  was more efficient as it uses median and interquartile range to normalise data rather than the mean, meaning it is less sensitive to outliers. This had a much more positive impact as shown in the figure below.\n",
    "\n",
    "Robust scale (x)=  (x-median(x))/(Q3(x)-Q1(x))\n",
    "\n",
    "omitted another figure\n",
    "\n",
    "In order to more swiftly train the model, variables that had a lesser outcome on the class would be considered as secondary features. There seemed to be much more emphasis on more primary ones such as ‘u’, ‘g’, ‘r’ , ‘i’, ‘z’, ‘alpha’, ‘delta’, ‘plate’ and especially ‘redshift’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603a38f-d8c2-4960-876d-e8881e04c218",
   "metadata": {},
   "source": [
    "## 1.2.3 Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c22cce-47b0-4a70-87ab-e96e56b3eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads dataset and puts it in a dataframe\n",
    "path = \".\"\n",
    "filename_read = os.path.join(path, \"heart_attack_prediction_dataset.csv\")\n",
    "df = pd.read_csv(filename_read)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
